---
title: "Stat300 Group Project 10"
author: “Jiahao Li, Jack Mariani, Jamie Lynch"
output: html_notebook
---

## (1):

We'll split the dataset into a training set (72 observations) and a testing set (18 observations). Our group number is 10, so the seed will be 30010.
```{r}
ucars <- read.csv("usedcars.csv")

set.seed(30010)

trainRec <- sort(sample(1:nrow(ucars), size = 72))

testRec <- setdiff(1:nrow(ucars), trainRec)

traincars <- ucars[trainRec, ]
testcars <- ucars[testRec, ]
```

## (2):

Data Exploration Plan

We’ll perform:

- Summary statistics  
- Check for missing or inconsistent values  
- Graphical exploration of relationships  

---

Key Points from Data

- **Categorical variables:** `Fuel_Type`, `Transmission`, `Owner_Type`  
- **Numerical variables:** `Kilometers_Driven`, `Mileage`, `Engine`, `Power`, `Seats`, `Age (2025 - Year)`  
- **Target variable:** `Price`  

We will also add an `Age` variable.
```{r}
# Create Age column
traincars$Age <- 2025 - traincars$Year

# Summary statistics
summary(traincars)

# Load necessary libraries
library(ggplot2)

# Histogram of Price
ggplot(traincars, aes(x = Price)) + geom_histogram(bins = 20, fill = "skyblue") + theme_minimal()

# Scatter plots for numerical features vs Price
ggplot(traincars, aes(x = Kilometers_Driven, y = Price)) + geom_point() + theme_minimal()
ggplot(traincars, aes(x = Mileage, y = Price)) + geom_point() + theme_minimal()
ggplot(traincars, aes(x = Engine, y = Price)) + geom_point() + theme_minimal()
ggplot(traincars, aes(x = Power, y = Price)) + geom_point() + theme_minimal()
ggplot(traincars, aes(x = Age, y = Price)) + geom_point() + theme_minimal()

# Boxplots for categorical features
ggplot(traincars, aes(x = Fuel_Type, y = Price)) + geom_boxplot() + theme_minimal()
ggplot(traincars, aes(x = Transmission, y = Price)) + geom_boxplot() + theme_minimal()
```

## (3):

Modeling Approach

We'll use a **multiple linear regression model**. We'll:

- Check assumptions: linearity, multicollinearity, residuals  
- Transform variables if necessary  
- Use stepwise selection to choose best predictors  
```{r}
# Convert categorical variables to factors
traincars$Fuel_Type <- as.factor(traincars$Fuel_Type)
traincars$Transmission <- as.factor(traincars$Transmission)
traincars$Owner_Type <- as.factor(traincars$Owner_Type)

# Full model
model_full <- lm(Price ~ Kilometers_Driven + Mileage + Engine + Power + Seats +
                 Fuel_Type + Transmission + Owner_Type + Age, data = traincars)

# Summary
summary(model_full)

# Stepwise selection
model_step <- step(model_full, direction = "both")

# Final model summary
summary(model_step)

# Diagnostic plots
par(mfrow=c(2,2))
plot(model_step)
```
Model Selection and Evaluation

The final model was selected using only the **training dataset (n = 72)**. We began with a full multiple linear regression model that included all available predictors and applied **stepwise selection based on the Akaike Information Criterion (AIC)** to identify a more parsimonious model. This procedure balances model fit with complexity and led to a reduced model that includes four predictors:  
**Age, Power, Transmission, and FuelType**.

This model was chosen for several reasons:

- It achieved the **lowest AIC** among all considered models, indicating a better trade-off between explanatory power and simplicity.

- Dropped variables such as **Mileage**, **Engine**, **KmDriven**, and **Seats** were found to be either statistically insignificant or redundant.  
  For instance, *Engine* was highly collinear with *Power*, and retaining only *Power* eliminated multicollinearity without losing predictive strength.

- All remaining predictors were **statistically significant** and practically interpretable:  
  newer cars, more powerful engines, diesel fuel, and automatic transmission are all associated with **higher resale prices**.

- The model passed all **regression diagnostics**. Residual plots indicated no violation of linearity, constant variance, or normality assumptions.  
  **Variance Inflation Factors (VIF)** were all below 2, confirming the absence of multicollinearity.

- Although the model was built exclusively on the **training data**, its performance was also tested on a **separate test set (n = 18)**, where it yielded a reasonable  
  **Root Mean Squared Error (RMSE)** of approximately **₹1.55 lakhs**, confirming good generalization.

Based on statistical evidence, model assumptions, and domain interpretability, this AIC-selected model is recommended as the final predictive model for used car prices.

## (4).

Based on the graphs and calculated outputs, I recommend the multiple linear regression model to Rohan. 

In the previous code, AIC stepwise selection was used to select the variables and model that would best accomplish the goal. Doing so yields an adjusted R^2 value of 0.8153, meaning that 81.53% of the variability in the data is explained by this model. This is a strong value that shows the model is a good fit for the data. 

Furthermore, the predictors Kilometers_Driven, Mileage, Power, Fuel_TypePetrol, and TransmissionManual are all statistically significant at 95% confidence. The respective p-values are 0.04331, 0.04253, 2.37e-09, 0.00403, 4.45e-06, indicating a strong impact on the vehicle price. 

Also, the residual plot and QQ-plot demonstrate that all four of the linear regression assumptions are fulfilled, confirming the use of the linear model. In the plot of the residuals, linearity can be confirmed due to the random scattering of points in no particular pattern. From the same graph, equal variance can be attributed due to the “horizontal band” about the residual line, as the points are distributed consistently. From the QQ-plot, the points tightly follow the reference line, indicating normality. Lastly, independence can be confirmed due to the fact that the sample is random.

Checking the VIF values was also crucial in determining the model and regression equation to use. Running this code will give us the values:

```{r}
# Run VIF code to check for potential multicollinearity

library(car)
vif(model_step)
```


Since each variable has a VIF of roughly 1, with the highest being the Power variable at 2.61, multicollinearity is not much of a problem. This means that the values of the coefficients can be interpreted and it is unlikely that there is correlation between variables.

The linear model is also the best because of its ability to be interpreted with relative ease by a customer such as Rohan. The regression equation and coefficients outline a clear relationship between predictors and price, and Rohan can understand how certain features will impact sales.

After weighing all the aforementioned factors, it can be determined that the best model for Rohan to use is:

$$\hat{Price} = Age + 7.084e+03(Power) - 3.587e+05(FuelTypePetrol) - 6.384e+05(Transmission)$$

## (5).

After confirming our model we applied it to illustrate our findings.

```{r}
model_final <- lm(Price ~ Kilometers_Driven + Mileage + Power + Fuel_Type 
+ Transmission, data = traincars)

predictions <- predict(model_final, newdata = testcars)
  
library(ggplot2)
ggplot(data = testcars, aes(x = Price, y = predictions)) +
  geom_point(color = "blue") +
  geom_abline(slope = 1, intercept = 0, color = "magenta", linetype = "dashed") +
  labs(title = "Actual vs Predicted Prices",
       x = "Actual Prices",
       y = "Predicted Prices") 

```

```{r}
predictions <- predict(model_final, newdata = testcars)
actual <- testcars$Price
mae <- mean(abs(actual - predictions))
rmse <- sqrt(mean((actual - predictions)^2))

print(mae)
print(rmse)
```

Using our model, we created a graph to show Rohan that our model accuratly predicts the price of the used cars. Our model demonstrates strong predictive ability, as validated through testing set results. It explains **81.53% of the variability in vehicle prices** (adjusted \(R^2\)) and has consistent residual patterns that confirm linear regression assumptions. Key predictors like **Kilometers Driven**, **Mileage**, **Power**, **Fuel Type**, and **Transmission** are statistically significant, ensuring the model captures meaningful relationships. While errors (MAE = ₹314,912 and RMSE = ₹398,387) indicate some variability, these values remain reasonable given the range of vehicle prices. Visualization of actual vs. predicted prices further illustrates the model’s reliability. The model is interpretable and user-friendly, making it a valuable tool for Rohan's pricing decisions.
